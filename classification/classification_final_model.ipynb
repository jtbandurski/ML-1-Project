{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors: Jakub Bandurski, Anirban Das"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook performs all necessary computations needed to generate predictions of the final model for the classification task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this file simply install all dependencies and run cell by cell.\n",
    "\n",
    "Input: train and test sets provided by the lecturer\n",
    "\n",
    "Output: final prediction, processed train and test sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "# preprocessing\n",
    "# explicitly require this experimental feature to run imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# model\n",
    "import xgboost as xgb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_23396\\3538125820.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_23396\\3538125820.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (10127, 31)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 0.09\n",
      "[IterativeImputer] Change: 3.112848513312019, scaled tolerance: 0.02489482411360622 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 0.17\n",
      "[IterativeImputer] Change: 1.0348486648124977, scaled tolerance: 0.02489482411360622 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 0.26\n",
      "[IterativeImputer] Change: 0.28828216068862855, scaled tolerance: 0.02489482411360622 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 0.34\n",
      "[IterativeImputer] Change: 0.07998240242181298, scaled tolerance: 0.02489482411360622 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 0.42\n",
      "[IterativeImputer] Change: 0.022189352081924962, scaled tolerance: 0.02489482411360622 \n",
      "[IterativeImputer] Early stopping criterion reached.\n"
     ]
    }
   ],
   "source": [
    "# import data set\n",
    "client_attrition = pd.read_csv('./../data/client_attrition_train.csv')\n",
    "# encode dependant varaible as 0 and 1\n",
    "client_attrition['account_status'] = client_attrition['account_status'].map(dict(closed=1, open=0))\n",
    "# drop target var\n",
    "y_train = client_attrition['account_status']\n",
    "client_attrition.drop('account_status',axis=1,inplace=True)\n",
    "# drop id\n",
    "client_attrition.drop(labels='customer_id',axis=1,inplace=True)\n",
    "# OneHotEncoding of not missing columns\n",
    "client_attrition = pd.get_dummies(client_attrition, columns=['customer_education', 'customer_civil_status','credit_card_classification'])\n",
    "# select numeric columns\n",
    "numeric_columns = list(client_attrition.select_dtypes(exclude = ['object']).columns)\n",
    "# select categorical columns\n",
    "categorical_columns = list(client_attrition.select_dtypes(include = ['object']).columns)\n",
    "# scaling before imputation\n",
    "client_attrition_scaled = (client_attrition[numeric_columns]-client_attrition[numeric_columns].mean())/client_attrition[numeric_columns].std()\n",
    "client_attrition_scaled['customer_sex'] = client_attrition['customer_sex']\n",
    "client_attrition_scaled['customer_salary_range'] = client_attrition['customer_salary_range']\n",
    "#instantiate both packages to use\n",
    "encoder = OrdinalEncoder()\n",
    "imputer = IterativeImputer(skip_complete=True,max_iter=10,verbose=2,random_state=12345)\n",
    "# create a list of categorical columns to iterate over\n",
    "cat_cols = categorical_columns\n",
    "\n",
    "def encode(data):\n",
    "    '''function to encode non-null data and replace it in the original data'''\n",
    "    #retains only non-null values\n",
    "    nonulls = np.array(data.dropna())\n",
    "    #reshapes the data for encoding\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    #encode date\n",
    "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
    "    #Assign back encoded values to non-null values\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    return data\n",
    "\n",
    "#create a for loop to iterate through each column in the data\n",
    "for columns in cat_cols:\n",
    "    encode(client_attrition_scaled[columns])\n",
    "# impute data and convert \n",
    "client_attrition_imputed = pd.DataFrame(imputer.fit_transform(client_attrition_scaled))\n",
    "client_attrition_imputed.columns = client_attrition_scaled.columns\n",
    "client_attrition_imputed[[\"customer_sex\",\"customer_salary_range\"]] = client_attrition_imputed[[\"customer_sex\",\"customer_salary_range\"]].round()\n",
    "# convert customer_sex and customer_salary_range to factors and then one hot encode\n",
    "client_attrition_imputed['customer_sex'] = client_attrition_imputed['customer_sex'].astype(int).astype(str)\n",
    "client_attrition_imputed['customer_sex'] = client_attrition_imputed['customer_sex'].map({'0':\"F\", '1':\"M\"})\n",
    "client_attrition_imputed['customer_salary_range'] = client_attrition_imputed['customer_salary_range'].astype(int).astype(str)\n",
    "client_attrition_imputed['customer_salary_range'] = client_attrition_imputed['customer_salary_range'].map({'0':\"120K and more\", '1':\"40-60K\", '2':'60-80K','3':'80-120K','4':'Uknown','5':'below 40K'})\n",
    "# One Hot Encode these columns\n",
    "client_attrition_imputed = pd.get_dummies(client_attrition_imputed, columns=['customer_sex', 'customer_salary_range'])\n",
    "# Scale again, already scaled columns wont be affected, imputed and new columns will, all columns are now numeric\n",
    "client_attrition_imputed_scaled = (client_attrition_imputed-client_attrition_imputed.mean())/client_attrition_imputed.std()\n",
    "# add y_train to train set\n",
    "client_attrition_imputed_scaled = pd.concat([y_train, client_attrition_imputed_scaled],axis=1)\n",
    "# save to csv\n",
    "client_attrition_imputed_scaled.to_csv('client_attrition_train_processed.csv',sep=\";\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.38705992698669434 seconds\n"
     ]
    }
   ],
   "source": [
    "y_train = client_attrition_imputed_scaled['account_status']\n",
    "X_train = client_attrition_imputed_scaled.drop('account_status',axis=1)\n",
    "\n",
    "# set hyperparameters\n",
    "params = {\n",
    "    'eta': 1,\n",
    "    'max_depth': 2,\n",
    "    'lambda': 1,\n",
    "    'min_child_weight': 5,\n",
    "    'gamma': 0.01,\n",
    "    'colsample_bytree': 1\n",
    "}\n",
    "# train model on train set\n",
    "start = time()\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "print(f\"Training time: {time()-start} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_23396\\2758116016.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_23396\\2758116016.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (5063, 31)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 0.06\n",
      "[IterativeImputer] Change: 2.4077439150829094, scaled tolerance: 0.07114070703550365 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 0.15\n",
      "[IterativeImputer] Change: 0.5850500650415553, scaled tolerance: 0.07114070703550365 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 0.20\n",
      "[IterativeImputer] Change: 0.09080384604186786, scaled tolerance: 0.07114070703550365 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 0.25\n",
      "[IterativeImputer] Change: 0.014075500465657309, scaled tolerance: 0.07114070703550365 \n",
      "[IterativeImputer] Early stopping criterion reached.\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "client_attrition_test = pd.read_csv('./../data/client_attrition_test.csv')\n",
    "# drop id\n",
    "client_attrition_test.drop(labels='customer_id',axis=1,inplace=True)\n",
    "# OneHotEncoding of not missing columns\n",
    "client_attrition_test = pd.get_dummies(client_attrition_test, columns=['customer_education', 'customer_civil_status','credit_card_classification'])\n",
    "# Add column of 0s for Platinium value\n",
    "client_attrition_test['credit_card_classification_Platinum'] = 0\n",
    "# select numeric columns\n",
    "numeric_columns_test = list(client_attrition_test.select_dtypes(exclude = ['object']).columns)\n",
    "# select categorical columns\n",
    "categorical_columns = list(client_attrition_test.select_dtypes(include = ['object']).columns)\n",
    "\n",
    "################################ knn?\n",
    "# scaling before imputation\n",
    "client_attrition_test_scaled = (client_attrition_test[numeric_columns_test]-client_attrition_test[numeric_columns_test].mean())/client_attrition_test[numeric_columns_test].std()\n",
    "client_attrition_test_scaled['credit_card_classification_Platinum'] = 0\n",
    "client_attrition_test_scaled['customer_sex'] = client_attrition_test['customer_sex']\n",
    "client_attrition_test_scaled['customer_salary_range'] = client_attrition_test['customer_salary_range']\n",
    "# Imputation process\n",
    "# initiate both functions to use\n",
    "encoder = OrdinalEncoder()\n",
    "imputer = IterativeImputer(skip_complete=True,max_iter=10,verbose=2,random_state=12345)\n",
    "# create a list of categorical columns to iterate over\n",
    "cat_cols = categorical_columns\n",
    "\n",
    "def encode(data):\n",
    "    '''function to encode non-null data and replace it in the original data'''\n",
    "    # retains only non-null values\n",
    "    nonulls = np.array(data.dropna())\n",
    "    # reshapes the data for encoding\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    # encode date\n",
    "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
    "    # Assign back encoded values to non-null values\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    return data\n",
    "\n",
    "# create a for loop to iterate through each column in the data\n",
    "for columns in cat_cols:\n",
    "    encode(client_attrition_test_scaled[columns])\n",
    "# impute data and convert \n",
    "client_attrition_test_imputed = pd.DataFrame(imputer.fit_transform(client_attrition_test_scaled))\n",
    "client_attrition_test_imputed.columns = client_attrition_test_scaled.columns\n",
    "client_attrition_test_imputed[[\"customer_sex\",\"customer_salary_range\"]] = client_attrition_test_imputed[[\"customer_sex\",\"customer_salary_range\"]].round()\n",
    "# convert customer_sex and customer_salary_range to factors and then one hot encode\n",
    "client_attrition_test_imputed['customer_sex'] = client_attrition_test_imputed['customer_sex'].astype(int).astype(str)\n",
    "client_attrition_test_imputed['customer_sex'] = client_attrition_test_imputed['customer_sex'].map({'0':\"F\", '1':\"M\"})\n",
    "client_attrition_test_imputed['customer_salary_range'] = client_attrition_test_imputed['customer_salary_range'].astype(int).astype(str)\n",
    "client_attrition_test_imputed['customer_salary_range'] = client_attrition_test_imputed['customer_salary_range'].map({'0':\"120K and more\", '1':\"40-60K\", '2':'60-80K','3':'80-120K','4':'Uknown','5':'below 40K'})\n",
    "client_attrition_test_imputed = pd.get_dummies(client_attrition_test_imputed, columns=['customer_sex', 'customer_salary_range'])\n",
    "# scale again, already scaled columns wont be affected, imputed and new columns will, all columns are now numeric\n",
    "client_attrition_test_imputed_scaled = (client_attrition_test_imputed-client_attrition_test_imputed.mean())/client_attrition_test_imputed.std()\n",
    "client_attrition_test_imputed_scaled['credit_card_classification_Platinum'] = 0\n",
    "# save processed data set\n",
    "client_attrition_test_imputed_scaled.to_csv('client_attrition_test_processed.csv',sep=\";\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct arrangemnt of columns\n",
    "cols_when_model_builds = model.get_booster().feature_names\n",
    "client_attrition_test_imputed_scaled = client_attrition_test_imputed_scaled[cols_when_model_builds]\n",
    "X_test = client_attrition_test_imputed_scaled\n",
    "# prediction\n",
    "y_pred = model.predict(X_test)\n",
    "# save prediction\n",
    "np.savetxt(\"classification_prediction.csv\", y_pred.astype(int), fmt='%i' , delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13114754098360656"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fraction of predicted 1s is similar to the train set 16%\n",
    "np.count_nonzero(y_pred)/y_pred.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
